<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Uber ATG&#39;s ML Infrastructure and Version Control Platform</title>
    <url>/2020/05/01/UberMLInfrastructureForAD/</url>
    <content><![CDATA[<p><a href="https://eng.uber.com/machine-learning-model-life-cycle-version-control/" target="_blank" rel="noopener">Original post link</a> and <a href="https://mp.weixin.qq.com/s/yJzW3-RLC0d05vW2EqjY0w" target="_blank" rel="noopener">Chinese version</a></p>
<p>Uber ATG: Advanced Technologies Group, developing self-driving vehicles technology.</p>
<a id="more"></a>
<p>This post gives a general introduction of the CI/CD tool design principle in Uber ATG and it’s related stuff, including: ML components, development life cycle of ML software, automating tool (VerCD)</p>
<h2 id="Self-driving-vehicle-ML-model-components"><a href="#Self-driving-vehicle-ML-model-components" class="headerlink" title="Self-driving vehicle ML model components"></a>Self-driving vehicle ML model components</h2><p>Software components (tasks) that contains one or more ML models</p>
<ul>
<li>Perception: Object identification (types: vehicle, pedestrian, bicycle, etc.) and 3D localization.</li>
<li>Prediction: Combine object detection results (type + localization) with HD maps, to perform object trajectory prediction in future n seconds in a given scene. The Prediction component allows the self-driving vehicle to anticipate where actors will most likely be located at various points in the future.</li>
<li>Motion Planning:<ul>
<li>Input:<ul>
<li>self-driving vehicle’s destination</li>
<li>the predicted trajectories of all actors in the scene</li>
<li>the high definition map</li>
<li>other mechanisms</li>
</ul>
</li>
<li>Output:<ul>
<li>plan the path of the vehicle.</li>
</ul>
</li>
</ul>
</li>
<li>Control: Path following for motion planned path by controlling steering, brakes and accelerator.</li>
</ul>
<h2 id="ML-model-life-cycle"><a href="#ML-model-life-cycle" class="headerlink" title="ML model life cycle."></a>ML model life cycle.</h2><p>Training data source: sensor-equipped vehicles(Lidar, cameras, radar) in a wide variety of traffic situations.<br>Labeling work: location, shape, other attributes like object type, etc.<br>Model training: sufficient labeled training data + HD map<br>ML Model different layers: highest layer: ML applications, middle layer: common ML libraries such as TF and PyTorch, GPU acceleration.<br>Five-step life cycle for training and deployment of ML models:<br><img src="ATG_ML_platform_figure-lifecycle.png" alt="Life cycle for training and deployment of ML models" title="Life cycle for training and deployment of ML models"></p>
<h3 id="Data-ingestion"><a href="#Data-ingestion" class="headerlink" title="Data ingestion"></a>Data ingestion</h3><p>Data ingestion process can be seen as two steps: <strong>Select the logs intended to use</strong> and <strong>Extract data from logs</strong>.</p>
<ol>
<li><p><strong>Data log selection and dividing</strong>:<br>75% training data, 15% testing data, 10% validation data. The pipeline used to select logs and split the data into 3 groups based on geographical location: GeoSplit.</p>
</li>
<li><p><strong>Data extraction from logs</strong> (Using Petastorm, Uber ATG’s open source data accesss library for DL):<br>2.1 Data contents:</p>
<ul>
<li>Images from the vehicle’s cameras</li>
<li>LiDAR 3D point information</li>
<li>Radar information</li>
<li>The state of the vehicle, including location, speed, acceleration and heading</li>
<li>Map information, such as the vehicle’s route and lanes it used</li>
<li>Ground truth labels</li>
</ul>
<p>2.2 Extraction process:</p>
<ul>
<li>extract data from multiple logs in parallel using Apache Spark (this way keeps the data in an optimized format by storing all info needed in GPUs memory after load from HDFS, so the training is efficient)</li>
<li>save data log-by-log on HDFS</li>
</ul>
</li>
</ol>
<p>Following picture shows the extraction process running on CPU cluster.<br><img src="ATG_ML_platform_figure-extraction.png" alt="Data Extraction Process" title="We use extracted data to run distributed training using Horovod on the GPU cluster and save the data to HDFS."></p>
<h3 id="Data-validation"><a href="#Data-validation" class="headerlink" title="Data validation"></a>Data validation</h3><p>Data validation follow the steps:</p>
<ol>
<li><strong>Run queries to pull</strong>: 1) the number of frames in a scene 2) the number of occurrences of the different label types.</li>
<li><strong>Compare results</strong> to previous data sets to see how conditions have changed. Further analysis needed if not as expected.</li>
</ol>
<h3 id="Model-training"><a href="#Model-training" class="headerlink" title="Model training"></a>Model training</h3><p>Distributed ML training system used: Horovod!</p>
<p>Distribute process:</p>
<ol>
<li><strong>Spread the training process</strong> on different GPUs with data parallelism as shown in figure below (same model trained on different GPUs with different parts of data). Each process performs forward and backword propagation independently.<br>[ATG_ML_platform_figure-modeltraining.png]</li>
<li><strong>Combine the parallel-trained model</strong>(knowledge distributing as I understand): using Horovod’s ring-allreduce algorithm to average gradients and disperse them to all work nodes without requiring a parameter server.</li>
<li><strong>Training verification</strong>: through TensorBoard, leveraging ML frameworks like TensorFlow and PyTorch.</li>
</ol>
<p>ML computing approach: hybrid! Both in on-premise data centers powered by GPU and CPU clusters as well as in the cloud.</p>
<ul>
<li><strong>On-premise data center</strong>: orchestrate training jobs using <a href="https://eng.uber.com/resource-scheduler-cluster-management-peloton/" target="_blank" rel="noopener">Peloton</a>, to scales jobs to GPUs and CPUs.</li>
<li><strong>Cloud-based training</strong>: using <a href="https://kubernetes.io/" target="_blank" rel="noopener">Kubernetes</a> to deploy and scale application containers across clusters.</li>
</ul>
<h3 id="Model-evaluation"><a href="#Model-evaluation" class="headerlink" title="Model evaluation"></a>Model evaluation</h3><p>Evaluate the performance on <strong>model itself</strong> (with model-specific metrics) and on <strong>entire system</strong> (with system metrics). Also use hardware metrics to evaluate the model <strong>running speed</strong> on hardware.</p>
<ol>
<li><strong>Model-specific metrics</strong><br>For example, Perception model: precision(correct detection rate) and recall(the proportion of the ground truth objects that model identified correctly).<br>For not performing well metrics, we adjust data pipeline by <a href="https://eng.uber.com/searchable-ground-truth-atg/" target="_blank" rel="noopener">including similar cases</a> (data augmentation). But prevent overfitting.</li>
<li><strong>System metrics</strong><br>Mainly safety and comfort measurement on overall vehicle motion, with large test set. Performed after model-specific metrics are well enough. Due to ML model dependencies, the system evaluation will give a comprehensive overview between components.</li>
<li><strong>Hardware metrics</strong><br>Using ATG’s internal benchmarking system to profile a certain part of software, and evaluate how quick it will run on vehicle hardware.</li>
</ol>
<h3 id="Model-serving"><a href="#Model-serving" class="headerlink" title="Model serving"></a>Model serving</h3><p>Deploy the model on an inference engine in vehicle with continuous iteration, focusing on the part need to be improved learned from model evaluation part.</p>
<h2 id="Accelerating-ML-research-with-CI-and-CD"><a href="#Accelerating-ML-research-with-CI-and-CD" class="headerlink" title="Accelerating ML research with CI and CD"></a>Accelerating ML research with CI and CD</h2><p>Continuous integration and delivery (VerCD platform) will wrap and automate the 5-step ML model developing process to an end-to-end workflow, reduce errors and boost the speed.</p>
<p>The VerCD platform is such a system that tracks dependencies between code, datasets and models throughout the workflow, starts with the dataset extraction stage, cover model training, and conclude with computing metrics.</p>
<p>Why developed Uber’s own platform:</p>
<ol>
<li>DL models have deep dependencies and development complexity. Although open source tools like Kubeflow and TensorFlow Extended provide high-level orchestration to build dataset and train models, they are not well integrated, and do not fully enable CD and CI</li>
<li>Traditional versioning and CD/CD tools like Git and Jenkins do not operate over ML artifacts.</li>
</ol>
<h3 id="The-need-for-agile-development-in-ML-workflows"><a href="#The-need-for-agile-development-in-ML-workflows" class="headerlink" title="The need for agile development in ML workflows"></a>The need for agile development in ML workflows</h3><p>The <a href="https://en.wikipedia.org/wiki/Agile_software_development" target="_blank" rel="noopener">standard agile development process</a> in application for ML workflows (what do they mean for ML):</p>
<ul>
<li><strong>Version controlling</strong><br>For ML it should allow the analysis of impact of a change on some part towards downstream dependencies, independently from one developer to another.</li>
<li><strong>Dependency tracking</strong><br>Dependency between data and results is: adjustments to labels and raw data changes the nature of the data set, which in turn affects training results. Moreover, the way of data extraction and model training code and configuration also affects the results. So the dependency tracking should tell that correct order: data extraction first then model building.</li>
<li><strong>Continuous integration and Continuous delivery</strong><br>The ML workflow involves <strong>code, data and models</strong>, of which only the first is handled by traditional software tools. Following figure illustrates some differences in workflow, and next figure shows the scope and complexity of system required to build a final ML artifact.<br><img src="ATG_ML_platform_figure-mlworkflow.png" alt="" title="The traditional continuous delivery cycle differs from that used for ML in that, instead of just building code and testing it, ML developers must also construct data sets, train models, and compute model metrics."><br><img src="%5BATG_ML_platform_figure-mlrequirementsforsystem.png" alt="" title="The final result of ML workflows is just a tiny artifact compared to all of the supporting systems and code, such as configuration, data collection, feature extraction, data verification, machine resource management, analysis tools, process management tools, serving infrastructure, and monitoring">(Source: Hidden Technical Debt in Machine Learning Systems. Used with permission in original ATG post.)</li>
</ul>
<h3 id="Deep-dependency-graphs-in-the-self-driving-domain"><a href="#Deep-dependency-graphs-in-the-self-driving-domain" class="headerlink" title="Deep dependency graphs in the self-driving domain"></a>Deep dependency graphs in the self-driving domain</h3><p>The deep dependency graph is a result of the layered architecture of self-driving software stack, where each layer provides a different ML function, shown in figure below. The dependency graph for an object detection model, shown on the left, and two other ML models, shown on the right, depicts code and configurations that are handled by version control systems (in green) and items that are not handled by version control (in grey).<br><img src="ATG_ML_platform_figure-dependencygraph.png" alt=""><br>3 ML layers:</p>
<ol>
<li><strong>object detection model</strong> whose input is raw sensor data,</li>
<li><strong>path prediction model</strong> whose input is the set of objects detected by the object detection model,</li>
<li><strong>planning model</strong> whose inputs are the outputs of the path prediction model.</li>
</ol>
<p>Each layer shows on left involves generating 3 artifacts:</p>
<ol>
<li>A <strong>data set</strong>, composed of raw source data, bounding boxes around different objects in the source image data (i.e. labels), and the data set generation code.</li>
<li>A <strong>trained model</strong>, which requires as input the data set artifact, the model training code, and configuration files governing model training.</li>
<li>A <strong>metrics report</strong>, which requires as input the trained model artifact, the data set, and the metrics generation code.</li>
</ol>
<p>Such deep dependencies brings particular challenge for CD.</p>
<h2 id="VerCD-for-Uber-ATG"><a href="#VerCD-for-Uber-ATG" class="headerlink" title="VerCD for Uber ATG"></a>VerCD for Uber ATG</h2><p>VerCD is a set of tools and microservices, to provide versioning and continuous delivery for Ml code and artifacts for Uber’s AD software. It is a platform with well integration on other components to empower existing orchestrators and guarantee the smooth end-to-end ML workflow.</p>
<p>VerCD offers a metadata service that tracks all dependencies of each ML compenents, include not only code but also data and model artifacts. This will ensure that ML artifacts are always reproducible and traceable for comparing new experiments against a historical baseline or track down bugs.</p>
<h3 id="VerCD-system-architecture"><a href="#VerCD-system-architecture" class="headerlink" title="VerCD system architecture"></a>VerCD system architecture</h3><p>VerCD is a set of separate microservices, <a href="https://eng.uber.com/building-tincup-microservice-implementation/" target="_blank" rel="noopener">architecture at Uber</a>, where each microservice is responsible for a function, so the system can easily scale. For CD/CI workflow, it’s linear and fixed, while for experimental workflow it’s more flexible.</p>
<p>The <strong>two main service modules</strong>, considers both <strong>experimental</strong>(process functions to build data sets, train models and run metrics) and <strong>production</strong> workflow(CD/CD driven by an orchestrator). So it must have interface for both humans and machines, for which we chose a REST APU with Python library bindings, show as below 2 figures.<br><img src="ATG_ML_platform_figure-07.png" alt="VerCD main architecture" title="VerCD consists of a version and dependency metadata service, and an orchestrator service. We use stock frameworks such as Flask, SQLAlchemy, MySQL, and Jenkins but augment their functionality with ATG-specific integrations."></p>
<h4 id="Version-and-Dependency-Metadata-Service"><a href="#Version-and-Dependency-Metadata-Service" class="headerlink" title="Version and Dependency Metadata Service"></a>Version and Dependency Metadata Service</h4><p><img src="ATG_ML_platform_figure-metadataservice.png" alt="Version and Dependency Metadata Service"><br>The Version and Dependency Metadata Service has individual endpoints for data set building, model training, and metrics computation. The REST API is a Flask and SQLAlchemy app, backed by MySQL to hold the dependency metadata. The yellow API handlers and data access layers were designed for ATG-specific use cases.</p>
<h4 id="Orchestrator-Service"><a href="#Orchestrator-Service" class="headerlink" title="Orchestrator Service"></a>Orchestrator Service</h4><p><img src="ATG_ML_platform_figure-orchestratorservice.png" alt="Orchestrator Service"><br>VerCD’s Orchestrator Service manages the workflow pipelines for building data sets, training models, and computing metrics. It is comprised of an off-the-shelf Jenkins distribution, augmented with our own ATG-specific integrations (yellow) that give the orchestrator the ability to interact with external ATG systems.</p>
<h4 id="Example-workflow"><a href="#Example-workflow" class="headerlink" title="Example workflow"></a>Example workflow</h4><ul>
<li><p><strong>Workflow</strong>:</p>
<ul>
<li>Dependencies of any dataset, model or metric builds are registered with VerCD, which manages the information in a database backend.</li>
<li>Then use a stock Jenkins orchestrator to do regular builds, and augment the function by providing connectors and integration code so it can interpret dependency metadata and operate ATG-specific systems (like call to build a runtime for testing, interact with code repo, create images with DL or Apache Spark lib, replicating datasets in between data centers and cloud, etc.).</li>
</ul>
</li>
<li><p><strong>Example</strong>: registering a new data set<br>-&gt; User registration a new data set<br>-&gt; Data set Service in VerCD stores the dependency metadata in system database<br>-&gt; The dependencies for data set, models and metrics will include the Git hash of the repo and a path to the code entry point<br>-&gt; The metadata service initiates a build with the orchestrator service, which kicks off an Apache Spark job to run the code, monitor the job, and finally replicate the data set to managed storage locations<br>-&gt; Then the metadata service is updated with the locations to which the data is replicated</p>
</li>
</ul>
<h3 id="Microservice-APIs"><a href="#Microservice-APIs" class="headerlink" title="Microservice APIs"></a>Microservice APIs</h3><p>How to ensure <strong>reproducibility</strong> and <strong>traceability</strong> for data set building, model training, and metrics runs: require all versioned immutable dependencies of the workflows to be specified during registration!</p>
<h4 id="Data-set-service-API"><a href="#Data-set-service-API" class="headerlink" title="Data set service API"></a>Data set service API</h4><ul>
<li><strong>Purpose</strong>: tracking the dependencies for building a given data set.</li>
<li><strong>How</strong>: using REST API to:<ul>
<li>Create new data set</li>
<li>Read metadata for a data set</li>
<li>Update the metadata of a data set</li>
<li>Delete a data set</li>
<li>Get the artifact locations of a data set (such as S3 or HDFS)</li>
</ul>
</li>
<li><strong>What to track to reproduce data artifacts</strong>:<ol>
<li>Sensor log IDs from the autonomous vehicle for each of train, test, and validation</li>
<li>Git hash of the code used to extract the data set from raw sensor data</li>
<li>Entry point for the extraction script</li>
<li>Metadata describing <strong>the lifecycle of the data set</strong>(status such as registered, build failed or aborted, build succeed, data set deprecated, or at end-of-life) and whether the specific version is the latest</li>
</ol>
</li>
</ul>
<h4 id="Model-service-API"><a href="#Model-service-API" class="headerlink" title="Model service API"></a>Model service API</h4><ul>
<li><strong>Purpose</strong>: tracking the dependencies for training a given model.</li>
<li><strong>How</strong>: using REST API to:<ul>
<li>Train a new model</li>
<li>Read the metadata for it</li>
<li>Update the metadata of a registered model</li>
<li>Promote to production</li>
</ul>
</li>
<li><strong>What to track to reproduce training runs</strong>:<ol>
<li>Versioned dataset as described in the previous section</li>
<li>Git hash of the code used for training</li>
<li>Entry point for the training script</li>
<li>Path to training config files</li>
<li>Path to the final trained artifacts</li>
<li>Metadata describing the lifecycle of the data set and whether the specific version is the latest</li>
</ol>
</li>
</ul>
<h4 id="Metrics-service-API"><a href="#Metrics-service-API" class="headerlink" title="Metrics service API"></a>Metrics service API</h4><ul>
<li><strong>Purpose</strong>: evaluation of trained models.</li>
<li><strong>How</strong>: using a metadata service to track the necessary dependencies reuqired to run the metrics job</li>
<li><strong>What to track to reproduce metrics</strong>:<ol>
<li>Versioned model as described in the previous section</li>
<li>Git hash of the code used for running the metric jobs</li>
<li>Entry point for the metrics running code</li>
<li>Path to config files</li>
<li>Metadata describing the lifecycle of the data set and whether the specific version is the latest</li>
</ol>
</li>
</ul>
<h4 id="Experimental-validation-and-production-workflows"><a href="#Experimental-validation-and-production-workflows" class="headerlink" title="Experimental, validation, and production workflows"></a>Experimental, validation, and production workflows</h4><h3 id="Data-set-and-model-onboarding-and-results"><a href="#Data-set-and-model-onboarding-and-results" class="headerlink" title="Data set and model onboarding and results"></a>Data set and model onboarding and results</h3>]]></content>
      <categories>
        <category>Automotive</category>
        <category>Autonomous Driving</category>
      </categories>
      <tags>
        <tag>AD</tag>
        <tag>AD platform</tag>
      </tags>
  </entry>
  <entry>
    <title>AI Platform Study</title>
    <url>/2020/05/01/AILearningArchitecture/</url>
    <content><![CDATA[<p>#</p>
<p>The development of AI technology is growing at an amazing pace, while to implement them from research into production (as most of the companies are doing nowadays), the barrier or bottleneck lays in between has become more clear: lacking of software infrastructure to continuously integrate, deploy, and improve the AI software.  </p>
<a id="more"></a>
<p>Why we need AI infrastructure:<br>As more machine learning platform released, the design and implementation of machine learning models becomes like just calling a function or using APIs. However the complete framework of an AI product should also need the AI infrastructure as the core part to scale up, which mainly focus on the DevOps that lots of AI researchers do not pay much attention to.<br>There are several headache issues we usually when building practical AI applications:</p>
<ul>
<li>computational resources are expensive</li>
<li>integration is hard and un-efficient as different toolchains have different interface</li>
<li>model training and turning (hyperparameter search) is difficult and time-consuming</li>
</ul>
<p>So I will try to read upon different kind of AI platform and framework and try to make a summary of various of framework, to gain a better understanding of the entire machine learning pipeline.</p>
<h2 id="Determined-AI-AI-Infrastructure-goes-Open-Source"><a href="#Determined-AI-AI-Infrastructure-goes-Open-Source" class="headerlink" title="Determined AI - AI Infrastructure goes Open Source"></a>Determined AI - AI Infrastructure goes Open Source</h2><p>News link: <a href="https://determined.ai/blog/ai-infrastructure-for-everyone/" target="_blank" rel="noopener">https://determined.ai/blog/ai-infrastructure-for-everyone/</a><br>Platform product webpage: <a href="https://determined.ai/" target="_blank" rel="noopener">https://determined.ai/</a><br>Github opensource repo: <a href="https://github.com/determined-ai/determined" target="_blank" rel="noopener">https://github.com/determined-ai/determined</a></p>
<p>Today I read about this deep learning training platform from Determined AI that helps to solve the DevOps headache for training models, share GPU resources and scale up the development.</p>
<p>Determined AI is a deep learning training platform that tries to bridges the gap between deep learning tools like TensorFlow and PyTorch. It tries to boost the process of DL as shown in the figure [loop.png]</p>
<p>The Determined Training Platform (<a href="https://determined.ai/developers/" target="_blank" rel="noopener">https://determined.ai/developers/</a>) tightly integrates following key features:</p>
<ul>
<li>High-performance distributed training: distrubuted training support built upon Horvod (distributed training framework), with optimization of twice the performance of Horvod. Distribute easily set-up without any additional changes to your model code, allows multiple users to seamlessly share the same GPU cluster.</li>
<li>State-of-the-art hyperparameter search: based on research results [reference 12345]. 100x faster than standard search methods and 10x faster than Bayesian Optimization methods.</li>
<li>DL tools for both individuals and teams: support experiment management with built-in experiment tracking, log management, metrics visualization, reproducibility, automatic fault tolerance for DL training jobs and dependency management.</li>
<li>Flexible GPU scheduling: including dynamically resizing training jobs on-the-fly and automatic management of cloud resources on AWS and GCP</li>
<li>Hardware-agnostic and integrated with the Open Source Ecosystem: support public cloud and on-prem infrastructure. Integrated with TensorBoard and GPU-powered Jupyter notebooks. See integrated tools diagram below:[determined-components.png]</li>
</ul>
<p>Most of the guidance can be found at Github page<br>Installation guidance: <a href="https://docs.determined.ai/latest/how-to/install-main.html" target="_blank" rel="noopener">https://docs.determined.ai/latest/how-to/install-main.html</a><br>Quick Start Guide: <a href="https://docs.determined.ai/latest/tutorials/quick-start.html" target="_blank" rel="noopener">https://docs.determined.ai/latest/tutorials/quick-start.html</a><br>Documentation: <a href="https://docs.determined.ai/latest/index.html" target="_blank" rel="noopener">https://docs.determined.ai/latest/index.html</a></p>
]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>LearningList</title>
    <url>/2020/03/23/LearningList/</url>
    <content><![CDATA[<p>A TODO list for study.</p>
<a id="more"></a>
<ul>
<li>Matlab sensor fusion and tracking e-book</li>
<li>Matlab autonomous driving toolbox [Zhihu post]</li>
<li>Finish Coursera ML course, data science course</li>
<li>Learn pytorch</li>
<li>Watch Guanqun’s videos</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Team Tesla VS Team Lidar</title>
    <url>/2020/03/07/ADSolutionArch/</url>
    <content><![CDATA[<p>Pure camera solution VS multi-sensor solution (heavily dependent on Lidar), which one is the correct way to go?</p>
]]></content>
      <categories>
        <category>Automotive</category>
        <category>Autonomous Driving</category>
      </categories>
      <tags>
        <tag>sensors</tag>
        <tag>AD</tag>
      </tags>
  </entry>
  <entry>
    <title>SLAM with Deep Learning - Summary</title>
    <url>/2020/03/06/DL-VSLAM-List/</url>
    <content><![CDATA[<p>A brief study list of some deep-learning-based vision SLAM methods.</p>
<a id="more"></a>

<h1 id="Ideas-of-using-deep-neural-network-in-SLAM"><a href="#Ideas-of-using-deep-neural-network-in-SLAM" class="headerlink" title="Ideas of using deep neural network in SLAM"></a>Ideas of using deep neural network in SLAM</h1><ul>
<li>Use CNN for image depth estimation</li>
<li>DNN to replace some traditional optimization problem, end to end solution, trained to perform better without loop closure, or reduce the load on loop closure part</li>
<li>Online learning and adaptation: how to generalize network power to unseen view</li>
<li>Self-tuning SLAM</li>
</ul>
<h1 id="Semantic-stereo-visual-odometry-Weiming’s-thesis"><a href="#Semantic-stereo-visual-odometry-Weiming’s-thesis" class="headerlink" title="Semantic stereo visual odometry (Weiming’s thesis)"></a>Semantic stereo visual odometry (Weiming’s thesis)</h1><p>Use CNN classification to recognize feature points as objects, and build a geometric map with semantic meanings for certain landmarks / objects.</p>
<p>Inspiration: semantic 3D reconstruction for map building, use classified objects as feature points</p>
<h1 id="DeepVO"><a href="#DeepVO" class="headerlink" title="DeepVO"></a>DeepVO</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>  End-to-end learning with deep Recurrent Convolutional Neural Networks<br>  Raw RGB image -&gt; pose<br>  CNN for image feature representation, RNN for pose learning</p>
<p>  Limits for traditional methods: need lots of engineering efforts to tune, scaling problem requires extra or prior knowledge</p>
<p>  Why need sequential data learning: VO is heavily rely on geometric features, dynamic motions among changes of images, rather than a single frame, so CNN is not enough</p>
<p>  How to generalise to new environment: using geometric feature representation learnt by CNN</p>
<h2 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h2><p>Two main types: geometry based and learning based</p>
<ul>
<li>Geometry based:<ul>
<li>Sparse feature based: outliers, noises cause drifts over time. Computational expensive. Keyframe based: PTAM, ORBSLAM.</li>
<li>Direct methods: photometric consistency, DTAM, SVO,</li>
</ul>
</li>
<li>Learning based: Using optical flow to train KNN (K nearest neighbour), GP (gaussian process) and SVM (support vector machines) for monocular VO [15,16,17].<ul>
<li>DL based methods: handle large scale, big data, non-linear high dimensional data</li>
</ul>
</li>
</ul>
<h2 id="3-RCNN-method"><a href="#3-RCNN-method" class="headerlink" title="3. RCNN method"></a>3. RCNN method</h2>]]></content>
      <categories>
        <category>SLAM</category>
        <category>Vision SLAM</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
        <tag>SLAM</tag>
        <tag>VSLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>A little corner</title>
    <url>/2020/03/06/AboutMyLove/</url>
    <content><![CDATA[<div id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="Oh, these decrypted content cannot be verified, but you can still have a look.">
  <div class="hbe-input-container">
  <input type="password" id="hbePass" placeholder="" />
    <label for="hbePass">Hey, password is required here.</label>
    <div class="bottom-line"></div>
  </div>
  <script id="hbeData" type="hbeData" data-hmacdigest="d72f86d47ccbc7dddc43e0ebbe8d909138fe2d8001c6ab5466204f01e011c57c">1f3a185c07e855eec305a4693d2f63e7ec827d6cb69ac16734b1a24d2ae35ed2883f51919d9cbd2dd23d7ff0b8d1a5ed985cab175b89e8f5302a04abc13d9e1d9a07aa9257fdd0b3727d0b0b0308ce22b66cdfd7370826231f2b1456fbee38a9633e9e664caac36f22b8b077d448cf075c713f8ed5709bc4a0a5156963391a237d56a9d20764afef0300736a2a54fd753683abb6f11377abfeaeebfc5960f08ef4c4d242570077af028c52766c78599b2cc0427bb7400137057e83ada8c78055f7aaa1581551f70b6572eab343919aedfdcef653bf9060c19d7b8cf7d60476c1dfb2b73b6ec262fa1e960ef2693a3417f8e8ab88723997dba518ea348bbc8587b116ba1044ceb177c875356e8a695b71b64f777c05f60e950ef2cf05e48665314fc6fce90ad745b2333d48e0a4b4e152fb38ca5dc87e70d3619482bdc516353cdc59cebd8b39c8a7bf9a7f58eb6d157c7b27b8073671c9606285852032c4cf491bb8b2b37d83394c54754a7fe27a7a5c26b0328a6580a64bcd74c5261cf7caa2d334f4ae9bd08a1729bc2bdf1209af23d21c0232ccf9c77b87f085ba07efecba7b68ff8a6bde3c92b9ed4642f6fc5c7e265d66ca6fff14d67ab759b6d08ddad15d4db2922282731ef13c5e65a1a4c148c24dbb66424bda51f1fe0c23a9874aa9476586e54121e1c466241e648848666f1bc0bf6e21c7013f186b5eb8c70839a3f8523a77fbb1caff175716324f3f2f76f5aeb19011335659085e498cf226e9b61b7184f39cfd80a151f6dde95802a401c428bedc80c05b70975763af4c6e13ba377dbd3f9644ae44d5f1f0db17e2d1b3bdb43f3c6939715cb0ebd1b7a6c49e21d6b18e955722af65205406d889c1d8056df5ee26a3ad2520cd29c5031a2dd0b4f194cc09733641710ec7bf25ef112b36</script>
</div>
<script src="/lib/blog-encrypt.js"></script><link href="/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      <categories>
        <category>Life &amp; Love</category>
      </categories>
  </entry>
  <entry>
    <title>Touching the Trend of Automotive Software Architecture</title>
    <url>/2020/03/06/AutomotiveSWArchIdeas/</url>
    <content><![CDATA[<p>A trend of going both modulation and centralization.<br>Inspiration from Tesla, autosar, Nvidia, ROS , android and SPA2</p>
]]></content>
      <categories>
        <category>Automotive</category>
        <category>Automotive Software</category>
      </categories>
      <tags>
        <tag>Software</tag>
      </tags>
  </entry>
  <entry>
    <title>Baidu Apollo Project</title>
    <url>/2020/03/05/BaiduApollo/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Automotive</category>
        <category>Autonomous Driving Platform</category>
      </categories>
      <tags>
        <tag>AD</tag>
        <tag>Nvidia</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning Study List</title>
    <url>/2020/03/05/MachineLearningStudyList/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Autonomous Driving Dataset Summary</title>
    <url>/2020/03/05/ADDataset/</url>
    <content><![CDATA[<p>Useful dataset collected for autonomous vehicle algorithm (perception, path planning and control). Mainly sensor raw dataset, some with labels.<br>Used for research, benchmarking, tranining, etc.</p>
<a id="more"></a>
<h1 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h1><ul>
<li>KITTI Dataset</li>
<li>Argoai Dataset</li>
<li>Waymo Open Dataset<h2 id="Dataset-Attributes-Comparison"><a href="#Dataset-Attributes-Comparison" class="headerlink" title="Dataset Attributes Comparison"></a>Dataset Attributes Comparison</h2><table>
<thead>
<tr>
<th>Dataset</th>
<th align="center">Scene for object tracking</th>
<th align="right">Map data</th>
</tr>
</thead>
<tbody><tr>
<td>KITTI</td>
<td align="center">22</td>
<td align="right">Nan</td>
</tr>
<tr>
<td>Argoai</td>
<td align="center">113 (3D annotated)</td>
<td align="right"><strong>HD map (2 cities)</strong></td>
</tr>
<tr>
<td>Waymo</td>
<td align="center">are neat</td>
<td align="right">Nan</td>
</tr>
</tbody></table>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> ...</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun</span><span class="params">()</span>:</span></span><br><span class="line">  <span class="keyword">return</span> ok</span><br></pre></td></tr></table></figure>

<h1 id="Argoai-Dataset"><a href="#Argoai-Dataset" class="headerlink" title="Argoai Dataset"></a>Argoai Dataset</h1><p>Official website: <a href="https://www.argoverse.org/data.html" target="_blank" rel="noopener">https://www.argoverse.org/data.html</a><br>Github: <a href="https://github.com/argoai/argoverse-api" target="_blank" rel="noopener">https://github.com/argoai/argoverse-api</a></p>
<p>Obviously the most unique part for Argoai dataset is the HD map data. It not only provides large amount of 3D annotated, well synced camera and lidar data, but also emphasis the importance of using HD map data in AD algorithm, and provides APIs to connect sensor perception data with the map data.</p>
<p>Argoai also provides two baseline for <a href="https://github.com/alliecc/argoverse_baselinetracker" target="_blank" rel="noopener">3D object tracking</a> and <a href="https://github.com/jagjeet-singh/argoverse-forecasting" target="_blank" rel="noopener">trajectory forecasting</a>.<br><img src="argoaisensorset.png" alt=""></p>
]]></content>
      <categories>
        <category>Dataset</category>
        <category>AD Dataset</category>
      </categories>
      <tags>
        <tag>Dataset</tag>
      </tags>
  </entry>
  <entry>
    <title>Nvidia DriveWorks Platform</title>
    <url>/2020/03/05/NvidiaDriveworksPlatform/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Automotive</category>
        <category>Autonomous Driving Platform</category>
      </categories>
      <tags>
        <tag>AD</tag>
        <tag>Nvidia</tag>
      </tags>
  </entry>
</search>
